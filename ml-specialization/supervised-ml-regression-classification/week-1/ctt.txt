cost function => untuk mengetahui seberapa jauh error antara y prediksi dan real
                 dalam fungsi untuk menentukan garis ( f(x) = wx + b )
cost func => 1/m * sum of (y hat - y)^2 or 1/2m * sum of (y hat - y)^2
y hat => hasil prediksi
y = real

diff cost func pertama dan kedua => kedua lebih sering dipakai di ml terutama supaya perhitungan lebih rapi

model => fx = wx + b
parameters => w, b
cost function => 1/2m * sum of (y hat - y)^2
goal => minimize J(w, b)

J(w) = 1/2m * sum of (fw(x) - y)^2

J(w) = 1/2m * sum of (wx - y)^2
    ^
    |
ketika b = 0

J(w) untuk menghitung total error dari model

================================================================

gradient descent algorithm
perlu diingat bahwa untuk menetapkan model kita menggunakan weight (w) dan bias (b)
untuk mengetahui error dari model pun, kita perlu w dan b
sehingga untuk menghitung langkah kemana kita akan pergi dalam menemukan min J, kita juga menghitung w dan b
formula di bawah akan terus diulang hingga ditemukan J(w) yang paling kecil atau mendekati 0

w = w - alpha * derivative / turunan dari J(w, b) => w - alpha * 1/m * sum of (f w,b(xi) - yi) * xi

b = b - alpha * derivative / turunan dari J(w, b) => b - alpha * 1/m * sum of (f w,b(xi) - yi)

alpha => learning rate => langkah dari pembelajaran algorithm