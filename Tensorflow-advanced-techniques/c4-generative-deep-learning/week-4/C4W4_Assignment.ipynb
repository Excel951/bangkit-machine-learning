{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROm8kovwJIxD"
      },
      "source": [
        "# Week 4 Assignment: GANs with Hands\n",
        "\n",
        "\n",
        "For the last programming assignment of this course, you will build a Generative Adversarial Network (GAN) that generates pictures of hands. These will be trained on a dataset of hand images doing sign language.\n",
        "\n",
        "The model you will build will be very similar to the DCGAN model that you saw in the second ungraded lab of this week. Feel free to review it in case you get stuck with any of the required steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6Oumw5-Jx1w"
      },
      "source": [
        "***Important:*** *This colab notebook has read-only access so you won't be able to save your changes. If you want to save your work periodically, please click `File -> Save a Copy in Drive` to create a copy in your account, then work from there.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0OwpFl8JIxP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k3nvoSP3Btzu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from IPython import display\n",
        "\n",
        "# 2.15.0\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxy_M7xbQef-"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cg_4z8-glz6P"
      },
      "outputs": [],
      "source": [
        "def plot_results(images, n_cols=None):\n",
        "    '''visualizes fake images'''\n",
        "    display.clear_output(wait=False)\n",
        "\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "\n",
        "    if images.shape[-1] == 1:\n",
        "        images = np.squeeze(images, axis=-1)\n",
        "\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "\n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI8bUNSJIxR"
      },
      "source": [
        "## Get the training data\n",
        "\n",
        "You will download the dataset and extract it to a directory in your workspace. As mentioned, these are images of human hands performing sign language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uIx-60V_BEyo"
      },
      "outputs": [],
      "source": [
        "# download the dataset\n",
        "training_url = \"https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/signs-training.zip\"\n",
        "training_file_name = \"signs-training.zip\"\n",
        "urllib.request.urlretrieve(training_url, training_file_name)\n",
        "\n",
        "# extract to local directory\n",
        "training_dir = \"./tmp\"\n",
        "zip_ref = zipfile.ZipFile(training_file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPZmV9RJIxR"
      },
      "source": [
        "## Preprocess the images\n",
        "\n",
        "Next, you will prepare the dataset to a format suitable for the model. You will read the files, convert it to a tensor of floats, then normalize the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4rf-e4f-d3H7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# mapping function for preprocessing the image files\n",
        "def map_images(file):\n",
        "  '''converts the images to floats and normalizes the pixel values'''\n",
        "  img = tf.io.decode_png(tf.io.read_file(file))\n",
        "  img = tf.dtypes.cast(img, tf.float32)\n",
        "  img = img / 255.0\n",
        "\n",
        "  return img\n",
        "\n",
        "# create training batches\n",
        "filename_dataset = tf.data.Dataset.list_files(\"/tmp/signs-training/*.png\")\n",
        "image_dataset = filename_dataset.map(map_images).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz9NfgdTJIxS"
      },
      "source": [
        "## Build the generator\n",
        "\n",
        "You are free to experiment but here is the recommended architecture:\n",
        "- *Dense*: number of units should equal `7 * 7 * 128`, input_shape takes in a list containing the random normal dimensions.\n",
        "    - `random_normal_dimensions` is a hyperparameter that defines how many random numbers in a vector you'll want to feed into the generator as a starting point for generating images.\n",
        "- *Reshape*: reshape the vector to a 7 x 7 x 128 tensor.\n",
        "- *BatchNormalization*\n",
        "- *Conv2DTranspose*: takes `64` units, kernel size is `5`, strides is `2`, padding is `SAME`, activation is `selu`.\n",
        "- *BatchNormalization*\n",
        "- *Conv2DTranspose*: `1` unit, kernel size is `5`, strides is `2`, padding is `SAME`, and activation is `tanh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uagZDaF0CZON"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You'll pass the random_normal_dimensions to the first dense layer of the generator\n",
        "random_normal_dimensions = 32\n",
        "\n",
        "### START CODE HERE ###\n",
        "generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=7 * 7 * 128, input_shape=[random_normal_dimensions]),\n",
        "    keras.layers.Reshape(target_shape=[7, 7, 128]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
        "])\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_lAy0bjJIxS"
      },
      "source": [
        "## Build the discriminator\n",
        "\n",
        "Here is the recommended architecture for the discriminator:\n",
        "- *Conv2D*: 64 units, kernel size of 5, strides of 2, padding is SAME, activation is a leaky relu with alpha of 0.2, input shape is 28 x 28 x 1\n",
        "- *Dropout*: rate is 0.4 (fraction of input units to drop)\n",
        "- *Conv2D*: 128 units, kernel size of 5, strides of 2, padding is SAME, activation is LeakyRelu with alpha of 0.2\n",
        "- *Dropout*: rate is 0.4.\n",
        "- *Flatten*\n",
        "- *Dense*: with 1 unit and a sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "siCh-qRtJIxT",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "discriminator = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same', activation=keras.layers.LeakyReLU(alpha=0.2), input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dropout(rate=0.4),\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same', activation=keras.layers.LeakyReLU(alpha=0.2)),\n",
        "    keras.layers.Dropout(rate=0.4),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKlTL1lhJIxT"
      },
      "source": [
        "## Compile the discriminator\n",
        "\n",
        "- Compile the discriminator with a binary_crossentropy loss and rmsprop optimizer.\n",
        "- Set the discriminator to not train on its weights (set its \"trainable\" field)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xh4EaHDlJIxT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "discriminator.trainable = False\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X25T2kUJIxT"
      },
      "source": [
        "## Build and compile the GAN model\n",
        "\n",
        "- Build the sequential model for the GAN, passing a list containing the generator and discriminator.\n",
        "- Compile the model with a binary cross entropy loss and rmsprop optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SBclsOMsJIxU"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "gan = keras.models.Sequential([generator, discriminator])\n",
        "gan.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX2CB0srJIxU"
      },
      "source": [
        "## Train the GAN\n",
        "\n",
        "Phase 1\n",
        "- real_batch_size: Get the batch size of the input batch (it's the zero-th dimension of the tensor)\n",
        "- noise: Generate the noise using `tf.random.normal`.  The shape is batch size x random_normal_dimension\n",
        "- fake images: Use the generator that you just created. Pass in the noise and produce fake images.\n",
        "- mixed_images: concatenate the fake images with the real images.\n",
        "  - Set the axis to 0.\n",
        "- discriminator_labels: Set to `0.` for fake images and `1.` for real images.\n",
        "- Set the discriminator as trainable.\n",
        "- Use the discriminator's `train_on_batch()` method to train on the mixed images and the discriminator labels.\n",
        "\n",
        "\n",
        "Phase 2\n",
        "- noise: generate random normal values with dimensions batch_size x random_normal_dimensions\n",
        "  - Use `real_batch_size`.\n",
        "- Generator_labels: Set to `1.` to mark the fake images as real\n",
        "  - The generator will generate fake images that are labeled as real images and attempt to fool the discriminator.\n",
        "- Set the discriminator to NOT be trainable.\n",
        "- Train the GAN on the noise and the generator labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AuV97d_kCpb_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_gan(gan, dataset, random_normal_dimensions, n_epochs=50):\n",
        "    \"\"\" Defines the two-phase training loop of the GAN\n",
        "    Args:\n",
        "      gan -- the GAN model which has the generator and discriminator\n",
        "      dataset -- the training set of real images\n",
        "      random_normal_dimensions -- dimensionality of the input to the generator\n",
        "      n_epochs -- number of epochs\n",
        "    \"\"\"\n",
        "\n",
        "    # get the two sub networks from the GAN model\n",
        "    generator, discriminator = gan.layers\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
        "        for real_images in dataset:\n",
        "\n",
        "### START CODE HERE ###\n",
        "            # infer batch size from the current batch of real images\n",
        "            real_batch_size = real_images.shape[0]\n",
        "\n",
        "            # Train the discriminator - PHASE 1\n",
        "            # Create the noise\n",
        "            noise = tf.random.normal(shape=[real_batch_size, random_normal_dimensions])\n",
        "\n",
        "            # Use the noise to generate fake images\n",
        "            fake_images = generator(noise)\n",
        "\n",
        "            # Create a list by concatenating the fake images with the real ones\n",
        "            mixed_images = tf.concat([fake_images, real_images], axis=0)\n",
        "\n",
        "            # Create the labels for the discriminator\n",
        "            # 0 for the fake images\n",
        "            # 1 for the real images\n",
        "            discriminator_labels = tf.constant([[0.]] * real_batch_size + [[1.]] * real_batch_size)\n",
        "\n",
        "            # Ensure that the discriminator is trainable\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            # Use train_on_batch to train the discriminator with the mixed images and the discriminator labels\n",
        "            discriminator.train_on_batch(mixed_images, discriminator_labels)\n",
        "\n",
        "            # Train the generator - PHASE 2\n",
        "            # create a batch of noise input to feed to the GAN\n",
        "            noise = tf.random.normal([real_batch_size, random_normal_dimensions])\n",
        "\n",
        "            # label all generated images to be \"real\"\n",
        "            generator_labels = tf.constant([[1.]] * real_batch_size)\n",
        "\n",
        "            # Freeze the discriminator\n",
        "            discriminator.trainable = False\n",
        "\n",
        "            # Train the GAN on the noise with the labels all set to be true\n",
        "            gan.train_on_batch(noise, generator_labels)\n",
        "\n",
        "### END CODE HERE ###\n",
        "        plot_results(fake_images, 16)\n",
        "        plt.show()\n",
        "    return fake_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzbX3hwKJIxW"
      },
      "source": [
        "### Run the training\n",
        "\n",
        "For each epoch, a set of 31 images will be displayed onscreen. The longer you train, the better your output fake images will be. You will pick your best images to submit to the grader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wYx9rzdACt0A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "WARNING:tensorflow:From d:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# run the training loop and collect images\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_normal_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[11], line 53\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(gan, dataset, random_normal_dimensions, n_epochs)\u001b[0m\n\u001b[0;32m     50\u001b[0m             discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;66;03m# Train the GAN on the noise with the labels all set to be true\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m             \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         plot_results(fake_images, \u001b[38;5;241m16\u001b[39m)\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\engine\\training.py:2779\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2777\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_on_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n\u001b[1;32m-> 2779\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   2781\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   2782\u001b[0m ):\n\u001b[0;32m   2783\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2784\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2785\u001b[0m     )\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\engine\\training.py:2723\u001b[0m, in \u001b[0;36mModel.reset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \n\u001b[0;32m   2706\u001b[0m \u001b[38;5;124;03mExamples:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2720\u001b[0m \n\u001b[0;32m   2721\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m-> 2723\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py:265\u001b[0m, in \u001b[0;36mMetric.reset_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\backend.py:4311\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   4309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[0;32m   4310\u001b[0m         value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39mdtype_numpy(x))\n\u001b[1;32m-> 4311\u001b[0m         \u001b[43m_assign_value_to_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4313\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\keras\\src\\backend.py:4359\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4356\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[0;32m   4357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4358\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4359\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1056\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1054\u001b[0m   validate_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_shape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;241m.\u001b[39mis_fully_defined()\n\u001b[0;32m   1055\u001b[0m   kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m validate_shape\n\u001b[1;32m-> 1056\u001b[0m assign_op \u001b[38;5;241m=\u001b[39m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_variable_op(\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, value_tensor, name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_read(assign_op)\n",
            "File \u001b[1;32md:\\Studi\\MBKM\\bangkit-machine-learning\\venv\\py_v_3_10_0\\tf2_15\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:149\u001b[0m, in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, validate_shape, name)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAssignVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    153\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# you can adjust the number of epochs\n",
        "EPOCHS = 60\n",
        "\n",
        "# run the training loop and collect images\n",
        "fake_images = train_gan(gan, image_dataset, random_normal_dimensions, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIAih3a1JIxX"
      },
      "source": [
        "## Choose your best images to submit for grading!\n",
        "\n",
        "Please visually inspect your 31 generated hand images.  They are indexed from 0 to 30, from left to right on the first row on top, and then continuing from left to right on the second row below it.\n",
        "\n",
        "- Choose 16 images that you think look most like actual hands.\n",
        "- Use the `append_to_grading_images()` function, pass in `fake_images` and a list of the indices for the 16 images that you choose to submit for grading (e.g. `append_to_grading_images(fake_images, [1, 4, 5, 6, 8... until you have 16 elements])`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qcxe1RK-piF"
      },
      "outputs": [],
      "source": [
        "# helper function to collect the images\n",
        "def append_to_grading_images(images, indexes):\n",
        "  l = []\n",
        "  for index in indexes:\n",
        "    if len(l) >= 16:\n",
        "      print(\"The list is full\")\n",
        "      break\n",
        "    l.append(tf.squeeze(images[index:(index+1),...], axis=0))\n",
        "  l = tf.convert_to_tensor(l)\n",
        "  return l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFg-wvIcS-Jv"
      },
      "source": [
        "Please fill in the empty list (2nd parameter) with 16 indices indicating the images you want to submit to the grader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InUSbfGI-0vk"
      },
      "outputs": [],
      "source": [
        "grading_images = append_to_grading_images(fake_images, [ ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsTurLWKJIxY"
      },
      "source": [
        "## Zip your selected images for grading\n",
        "\n",
        "Please run the code below. This will save the images you chose to a zip file named `my-signs.zip`.\n",
        "\n",
        "- Please download this file from the Files explorer on the left.\n",
        "- Please return to the Coursera classroom and upload the zip file for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL8W2OGBqFL_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "\n",
        "denormalized_images = grading_images * 255\n",
        "denormalized_images = tf.dtypes.cast(denormalized_images, dtype = tf.uint8)\n",
        "\n",
        "file_paths = []\n",
        "\n",
        "for this_image in range(0,16):\n",
        "  i = tf.reshape(denormalized_images[this_image], [28,28])\n",
        "  im = Image.fromarray(i.numpy())\n",
        "  im = im.convert(\"L\")\n",
        "  filename = \"hand\" + str(this_image) + \".png\"\n",
        "  file_paths.append(filename)\n",
        "  im.save(filename)\n",
        "\n",
        "with ZipFile('my-signs.zip', 'w') as zip:\n",
        "  for file in file_paths:\n",
        "    zip.write(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp7jYkyXZsM9"
      },
      "source": [
        "**Congratulations on completing the final assignment of this course!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
